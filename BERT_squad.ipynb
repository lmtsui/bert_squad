{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT-squad.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "295c7ae5ccb3479db0a918ef46664718": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8308a18044d5487ea7c174375d34dc32",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d4efb28d50164ab19211ef7921fc2ee2",
              "IPY_MODEL_546762858ab7438586799f88bb3b4c96"
            ]
          }
        },
        "8308a18044d5487ea7c174375d34dc32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d4efb28d50164ab19211ef7921fc2ee2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3fd8298ddd344be8b22c516f2c781424",
            "_dom_classes": [],
            "description": "Iteration",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 7470,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 7470,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1bb9d2b6123e41a6ad6cf17ba77a9902"
          }
        },
        "546762858ab7438586799f88bb3b4c96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_70381c5193734167b82dcf8627ea6a74",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 7470/7470 [1:04:50&lt;00:00,  2.34it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d5519cd14d884dd3b7a49e794f445131"
          }
        },
        "3fd8298ddd344be8b22c516f2c781424": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1bb9d2b6123e41a6ad6cf17ba77a9902": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "70381c5193734167b82dcf8627ea6a74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d5519cd14d884dd3b7a49e794f445131": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "40156f679b2f492ca15f2d8354ab5298": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_bd8845d26f894b16a6240132d5e62632",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_dd0705e24c764a1ca34b42f7a7b79201",
              "IPY_MODEL_db56250357fd4f9caa4bb6d9763e4065"
            ]
          }
        },
        "bd8845d26f894b16a6240132d5e62632": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dd0705e24c764a1ca34b42f7a7b79201": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_99510af820634b419e0dabe01f1bf59f",
            "_dom_classes": [],
            "description": "Iteration",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 7470,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 7470,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f387b3cbc14042bd9dbcc9925733918a"
          }
        },
        "db56250357fd4f9caa4bb6d9763e4065": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c6275d3a872e4104913bb045afc3314f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 7470/7470 [1:04:51&lt;00:00,  2.35it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bbe299e2f0a74e59a052c1881f08909c"
          }
        },
        "99510af820634b419e0dabe01f1bf59f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f387b3cbc14042bd9dbcc9925733918a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c6275d3a872e4104913bb045afc3314f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bbe299e2f0a74e59a052c1881f08909c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b58fd82446a143c18f2ef954693b80ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_014acd80b7ab4a6d8c2d375eb7615972",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fdc62dfb7ece40649976fb3e4acaa12b",
              "IPY_MODEL_8275eb400e7543cebce9dbc031b66e81"
            ]
          }
        },
        "014acd80b7ab4a6d8c2d375eb7615972": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fdc62dfb7ece40649976fb3e4acaa12b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_877bb321aa8a466491b072f05691abe7",
            "_dom_classes": [],
            "description": "Evaluating",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1372,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1372,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2e2b2c724f5a4afa93bb7fbfb623eca2"
          }
        },
        "8275eb400e7543cebce9dbc031b66e81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_690b2c84c6d0429caddfa9ba8e5eed23",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 1372/1372 [02:42&lt;00:00,  8.43it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4efeb67541e24d3db5f2bfc31e5e4033"
          }
        },
        "877bb321aa8a466491b072f05691abe7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2e2b2c724f5a4afa93bb7fbfb623eca2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "690b2c84c6d0429caddfa9ba8e5eed23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4efeb67541e24d3db5f2bfc31e5e4033": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kx_BanwRu-PG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/huggingface/transformers/blob/master/examples/run_squad.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bn_B25-GTFBt",
        "colab_type": "code",
        "outputId": "6807dd28-a80c-4a38-d3b1-2ef6098f3661",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n",
            "\r\u001b[K     |▊                               | 10kB 20.4MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30kB 3.1MB/s eta 0:00:01\r\u001b[K     |███                             | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |███▋                            | 51kB 2.6MB/s eta 0:00:01\r\u001b[K     |████▍                           | 61kB 3.0MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 71kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 81kB 3.9MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 92kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 102kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 112kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 122kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 133kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 143kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 153kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 163kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 174kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 184kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 194kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 204kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 215kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 225kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 235kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 245kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 256kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 266kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 276kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 286kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 296kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 307kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 317kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 327kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 337kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 348kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 358kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 368kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 378kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 389kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 399kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 409kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 419kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 430kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 440kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 450kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.47)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 38.1MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 31.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.47)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers) (2.6.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884629 sha256=88bfad7e6f226797985a0bfbe119d888d0a13f595a8f72c08e077aaab58c1acd\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 transformers-2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWRlNmsfRs0m",
        "colab_type": "code",
        "outputId": "dcd1a4c8-c7b9-4d1a-ba36-9ccfbff7b5c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        }
      },
      "source": [
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json https://rawcdn.githack.com/allenai/bi-att-flow/49004549e9a88b78c359b31481afa7792dbb3f4a/squad/evaluate-v1.1.py"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-15 15:14:06--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 30288272 (29M) [application/json]\n",
            "Saving to: ‘train-v1.1.json’\n",
            "\n",
            "train-v1.1.json     100%[===================>]  28.88M  59.0MB/s    in 0.5s    \n",
            "\n",
            "2020-01-15 15:14:07 (59.0 MB/s) - ‘train-v1.1.json’ saved [30288272/30288272]\n",
            "\n",
            "--2020-01-15 15:14:07--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\n",
            "Reusing existing connection to rajpurkar.github.io:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4854279 (4.6M) [application/json]\n",
            "Saving to: ‘dev-v1.1.json’\n",
            "\n",
            "dev-v1.1.json       100%[===================>]   4.63M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2020-01-15 15:14:07 (171 MB/s) - ‘dev-v1.1.json’ saved [4854279/4854279]\n",
            "\n",
            "--2020-01-15 15:14:07--  https://rawcdn.githack.com/allenai/bi-att-flow/49004549e9a88b78c359b31481afa7792dbb3f4a/squad/evaluate-v1.1.py\n",
            "Resolving rawcdn.githack.com (rawcdn.githack.com)... 172.64.109.8, 172.64.108.8, 2606:4700:e0::ac40:6c08, ...\n",
            "Connecting to rawcdn.githack.com (rawcdn.githack.com)|172.64.109.8|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/plain]\n",
            "Saving to: ‘evaluate-v1.1.py’\n",
            "\n",
            "evaluate-v1.1.py        [ <=>                ]   3.34K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-01-15 15:14:08 (56.3 MB/s) - ‘evaluate-v1.1.py’ saved [3419]\n",
            "\n",
            "FINISHED --2020-01-15 15:14:08--\n",
            "Total wall clock time: 1.7s\n",
            "Downloaded: 3 files, 34M in 0.5s (64.8 MB/s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFjy-KkkvFol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Args():\n",
        "    model_type = 'bert'\n",
        "    model_name_or_path = 'bert-base-cased'\n",
        "    output_dir = 'output/'\n",
        "    train_file = 'train-v1.1.json'\n",
        "    predict_file = 'dev-v1.1.json'\n",
        "    max_seq_length = 384\n",
        "    doc_stride = 128\n",
        "    max_query_length = 64\n",
        "    evaluate_during_training = False\n",
        "    per_gpu_train_batch_size = 12\n",
        "    per_gpu_eval_batch_size = 8\n",
        "    learning_rate = 3e-5\n",
        "    gradient_accumulation_steps = 1\n",
        "    weight_decay = 0.0\n",
        "    adam_epsilon = 1e-8\n",
        "    max_grad_norm = 1.0\n",
        "    num_train_epochs = 2.0\n",
        "    warmup_steps = 0\n",
        "    n_best_size = 20\n",
        "    max_answer_length = 30\n",
        "    logging_steps = 50\n",
        "    save_steps = 1000\n",
        "    no_cuda = False\n",
        "    seed = 42\n",
        "args = Args()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B1QgkWicBE_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        },
        "outputId": "54463bcf-b196-4a30-be0a-cb4fa81dc5c9"
      },
      "source": [
        "import glob\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import timeit\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "# from tqdm import tqdm, trange\n",
        "from tqdm import trange\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "from transformers import (\n",
        "    WEIGHTS_NAME,\n",
        "    AdamW,\n",
        "    BertConfig,\n",
        "    BertForQuestionAnswering,\n",
        "    BertTokenizer,\n",
        "    get_linear_schedule_with_warmup,\n",
        "    squad_convert_examples_to_features,\n",
        ")\n",
        "\n",
        "from transformers.data.metrics.squad_metrics import (\n",
        "    compute_predictions_log_probs,\n",
        "    compute_predictions_logits,\n",
        "    squad_evaluate,\n",
        ")\n",
        "from transformers.data.processors.squad import SquadResult, SquadV1Processor, SquadV2Processor\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F81geyp3ZoWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "MODEL_CLASSES = {\n",
        "    \"bert\": (BertConfig, BertForQuestionAnswering, BertTokenizer)\n",
        "}\n",
        "\n",
        "def set_seed(args):\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "\n",
        "def to_list(tensor):\n",
        "    return tensor.detach().cpu().tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUYVDxztvOTW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setup CUDA, GPU & distributed training\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "args.n_gpu = torch.cuda.device_count()\n",
        "args.device = device"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSFyqNZE2Ifw",
        "colab_type": "code",
        "outputId": "da4c3bc8-1089-4522-944b-29e81b9473f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        " # Setup logging\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        ")\n",
        "logger.warning(\n",
        "    \"Device: %s, n_gpu: %s\",\n",
        "    device,\n",
        "    args.n_gpu,\n",
        ")\n",
        "\n",
        "# Set seed\n",
        "set_seed(args)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "01/15/2020 15:15:08 - WARNING - __main__ -   Device: cuda, n_gpu: 1\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-GAZX9u2KvM",
        "colab_type": "code",
        "outputId": "ddeec4ff-97a5-4486-f842-50bdb773cf3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 971
        }
      },
      "source": [
        "args.model_type = args.model_type.lower()\n",
        "config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
        "config = config_class.from_pretrained(\n",
        "    args.model_name_or_path,\n",
        "    cache_dir=None,\n",
        ")\n",
        "tokenizer = tokenizer_class.from_pretrained(\n",
        "    args.model_name_or_path,\n",
        "    do_lower_case=True,\n",
        "    cache_dir=None,\n",
        ")\n",
        "model = model_class.from_pretrained(\n",
        "    args.model_name_or_path,\n",
        "    from_tf=False,\n",
        "    config=config,\n",
        "    cache_dir=None,\n",
        ")\n",
        "\n",
        "model.to(args.device)\n",
        "\n",
        "logger.info(\"Training/evaluation parameters %s\", args)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "01/15/2020 15:15:10 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json not found in cache or force_download set to True, downloading to /tmp/tmpz_h3vj09\n",
            "01/15/2020 15:15:11 - INFO - transformers.file_utils -   copying /tmp/tmpz_h3vj09 to cache at /root/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.d7a3af18ce3a2ab7c0f48f04dc8daff45ed9a3ed333b9e9a79d012a0dedf87a6\n",
            "01/15/2020 15:15:11 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.d7a3af18ce3a2ab7c0f48f04dc8daff45ed9a3ed333b9e9a79d012a0dedf87a6\n",
            "01/15/2020 15:15:11 - INFO - transformers.file_utils -   removing temp file /tmp/tmpz_h3vj09\n",
            "01/15/2020 15:15:11 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /root/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.d7a3af18ce3a2ab7c0f48f04dc8daff45ed9a3ed333b9e9a79d012a0dedf87a6\n",
            "01/15/2020 15:15:11 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "01/15/2020 15:15:11 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmp0oojpjvg\n",
            "01/15/2020 15:15:11 - INFO - transformers.file_utils -   copying /tmp/tmp0oojpjvg to cache at /root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
            "01/15/2020 15:15:11 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
            "01/15/2020 15:15:11 - INFO - transformers.file_utils -   removing temp file /tmp/tmp0oojpjvg\n",
            "01/15/2020 15:15:11 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
            "01/15/2020 15:15:11 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmp292kw6hj\n",
            "01/15/2020 15:15:20 - INFO - transformers.file_utils -   copying /tmp/tmp292kw6hj to cache at /root/.cache/torch/transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n",
            "01/15/2020 15:15:21 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n",
            "01/15/2020 15:15:21 - INFO - transformers.file_utils -   removing temp file /tmp/tmp292kw6hj\n",
            "01/15/2020 15:15:22 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /root/.cache/torch/transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n",
            "01/15/2020 15:15:25 - INFO - transformers.modeling_utils -   Weights of BertForQuestionAnswering not initialized from pretrained model: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "01/15/2020 15:15:25 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "01/15/2020 15:15:33 - INFO - __main__ -   Training/evaluation parameters <__main__.Args object at 0x7ff13e706518>\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAQaXoZki79a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False):\n",
        "    # Load data features from cache or dataset file\n",
        "    input_dir = \".\"\n",
        "    cached_features_file = os.path.join(\n",
        "        input_dir,\n",
        "        \"cached_{}_{}_{}\".format(\n",
        "            \"dev\" if evaluate else \"train\",\n",
        "            list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n",
        "            str(args.max_seq_length),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Init features and dataset from cache if it exists\n",
        "    if os.path.exists(cached_features_file):\n",
        "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "        features_and_dataset = torch.load(cached_features_file)\n",
        "        features, dataset, examples = (\n",
        "            features_and_dataset[\"features\"],\n",
        "            features_and_dataset[\"dataset\"],\n",
        "            features_and_dataset[\"examples\"],\n",
        "        )\n",
        "    else:\n",
        "        logger.info(\"Creating features from dataset file at %s\", input_dir)\n",
        "\n",
        "        processor = SquadV1Processor()\n",
        "        if evaluate:\n",
        "            examples = processor.get_dev_examples(filename=args.predict_file)\n",
        "        else:\n",
        "            examples = processor.get_train_examples(filename=args.train_file)\n",
        "\n",
        "        features, dataset = squad_convert_examples_to_features(\n",
        "            examples=examples,\n",
        "            tokenizer=tokenizer,\n",
        "            max_seq_length=args.max_seq_length,\n",
        "            doc_stride=args.doc_stride,\n",
        "            max_query_length=args.max_query_length,\n",
        "            is_training=not evaluate,\n",
        "            return_dataset=\"pt\",\n",
        "        )\n",
        "\n",
        "        logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
        "        torch.save({\"features\": features, \"dataset\": dataset, \"examples\": examples}, cached_features_file)\n",
        "\n",
        "    if output_examples:\n",
        "        return dataset, examples, features\n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_jGrnr8vyAP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "8a7e4ef2-f983-4d9f-a3e8-784ba6836a62"
      },
      "source": [
        "train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "01/15/2020 15:25:42 - INFO - __main__ -   Creating features from dataset file at .\n",
            "100%|██████████| 442/442 [00:33<00:00, 11.45it/s]\n",
            "Converting examples to features: 100%|██████████| 87599/87599 [14:21<00:00, 101.69it/s]\n",
            "01/15/2020 15:40:41 - INFO - __main__ -   Saving features into cached file ./cached_train_bert-base-cased_384\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdIQ2XIUSNIL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(args, train_dataset, model, tokenizer):\n",
        "    \"\"\" Train the model \"\"\"\n",
        "    tb_writer = SummaryWriter()\n",
        "\n",
        "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
        "    train_sampler = RandomSampler(train_dataset)\n",
        "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n",
        "\n",
        "    t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
        "\n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": args.weight_decay,\n",
        "        },\n",
        "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
        "    )\n",
        "\n",
        "    # Train!\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
        "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
        "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
        "    logger.info(\n",
        "        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
        "        args.train_batch_size\n",
        "        * args.gradient_accumulation_steps,\n",
        "    )\n",
        "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
        "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
        "\n",
        "    global_step = 1\n",
        "    epochs_trained = 0\n",
        "\n",
        "    tr_loss, logging_loss = 0.0, 0.0\n",
        "    model.zero_grad()\n",
        "    train_iterator = trange(\n",
        "        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=False\n",
        "    )\n",
        "    # Added here for reproductibility\n",
        "    set_seed(args)\n",
        "\n",
        "    for _ in train_iterator:\n",
        "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=False)\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "\n",
        "            model.train()\n",
        "            batch = tuple(t.to(args.device) for t in batch)\n",
        "\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"attention_mask\": batch[1],\n",
        "                \"token_type_ids\": batch[2],\n",
        "                \"start_positions\": batch[3],\n",
        "                \"end_positions\": batch[4],\n",
        "            }\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            # model outputs are always tuple in transformers (see doc)\n",
        "            loss = outputs[0]\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "\n",
        "                optimizer.step()\n",
        "                scheduler.step()  # Update learning rate schedule\n",
        "                model.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "                # Log metrics\n",
        "                if args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
        "                    # Only evaluate when single GPU otherwise metrics may not average well\n",
        "                    if args.evaluate_during_training:\n",
        "                        results = evaluate(args, model, tokenizer)\n",
        "                        for key, value in results.items():\n",
        "                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n",
        "                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
        "                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
        "                    logging_loss = tr_loss\n",
        "\n",
        "                # Save model checkpoint\n",
        "                if  args.save_steps > 0 and global_step % args.save_steps == 0:\n",
        "                    output_dir = os.path.join(args.output_dir, \"checkpoint-{}\".format(global_step))\n",
        "                    if not os.path.exists(output_dir):\n",
        "                        os.makedirs(output_dir)\n",
        "                    # Take care of distributed/parallel training\n",
        "                    model_to_save = model.module if hasattr(model, \"module\") else model\n",
        "                    model_to_save.save_pretrained(output_dir)\n",
        "                    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
        "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
        "\n",
        "                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
        "                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
        "                    logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n",
        "\n",
        "    tb_writer.close()\n",
        "\n",
        "    return global_step, tr_loss / global_step"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2EMABVP2M7F",
        "colab_type": "code",
        "outputId": "1f67fe9f-d9ea-4848-9619-766c7f0da54a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "295c7ae5ccb3479db0a918ef46664718",
            "8308a18044d5487ea7c174375d34dc32",
            "d4efb28d50164ab19211ef7921fc2ee2",
            "546762858ab7438586799f88bb3b4c96",
            "3fd8298ddd344be8b22c516f2c781424",
            "1bb9d2b6123e41a6ad6cf17ba77a9902",
            "70381c5193734167b82dcf8627ea6a74",
            "d5519cd14d884dd3b7a49e794f445131",
            "40156f679b2f492ca15f2d8354ab5298",
            "bd8845d26f894b16a6240132d5e62632",
            "dd0705e24c764a1ca34b42f7a7b79201",
            "db56250357fd4f9caa4bb6d9763e4065",
            "99510af820634b419e0dabe01f1bf59f",
            "f387b3cbc14042bd9dbcc9925733918a",
            "c6275d3a872e4104913bb045afc3314f",
            "bbe299e2f0a74e59a052c1881f08909c"
          ]
        }
      },
      "source": [
        "# Training\n",
        "  global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
        "  logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "01/14/2020 21:17:37 - INFO - __main__ -   Training/evaluation parameters <__main__.Args object at 0x7f2b799ab518>\n",
            "01/14/2020 21:17:37 - INFO - __main__ -   Loading features from cached file ./cached_train_bert-base-cased_384\n",
            "01/14/2020 21:18:03 - INFO - __main__ -   ***** Running training *****\n",
            "01/14/2020 21:18:03 - INFO - __main__ -     Num examples = 89632\n",
            "01/14/2020 21:18:03 - INFO - __main__ -     Num Epochs = 2\n",
            "01/14/2020 21:18:03 - INFO - __main__ -     Instantaneous batch size per GPU = 12\n",
            "01/14/2020 21:18:03 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 12\n",
            "01/14/2020 21:18:03 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "01/14/2020 21:18:03 - INFO - __main__ -     Total optimization steps = 14940\n",
            "\n",
            "\n",
            "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "295c7ae5ccb3479db0a918ef46664718",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Iteration', max=7470, style=ProgressStyle(description_width='…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "01/14/2020 21:26:38 - INFO - transformers.configuration_utils -   Configuration saved in tmp/debug_squad/checkpoint-1000/config.json\n",
            "01/14/2020 21:26:39 - INFO - transformers.modeling_utils -   Model weights saved in tmp/debug_squad/checkpoint-1000/pytorch_model.bin\n",
            "01/14/2020 21:26:39 - INFO - __main__ -   Saving model checkpoint to tmp/debug_squad/checkpoint-1000\n",
            "01/14/2020 21:26:43 - INFO - __main__ -   Saving optimizer and scheduler states to tmp/debug_squad/checkpoint-1000\n",
            "01/14/2020 21:35:20 - INFO - transformers.configuration_utils -   Configuration saved in tmp/debug_squad/checkpoint-2000/config.json\n",
            "01/14/2020 21:35:21 - INFO - transformers.modeling_utils -   Model weights saved in tmp/debug_squad/checkpoint-2000/pytorch_model.bin\n",
            "01/14/2020 21:35:21 - INFO - __main__ -   Saving model checkpoint to tmp/debug_squad/checkpoint-2000\n",
            "01/14/2020 21:35:24 - INFO - __main__ -   Saving optimizer and scheduler states to tmp/debug_squad/checkpoint-2000\n",
            "01/14/2020 21:44:00 - INFO - transformers.configuration_utils -   Configuration saved in tmp/debug_squad/checkpoint-3000/config.json\n",
            "01/14/2020 21:44:01 - INFO - transformers.modeling_utils -   Model weights saved in tmp/debug_squad/checkpoint-3000/pytorch_model.bin\n",
            "01/14/2020 21:44:01 - INFO - __main__ -   Saving model checkpoint to tmp/debug_squad/checkpoint-3000\n",
            "01/14/2020 21:44:05 - INFO - __main__ -   Saving optimizer and scheduler states to tmp/debug_squad/checkpoint-3000\n",
            "01/14/2020 21:52:41 - INFO - transformers.configuration_utils -   Configuration saved in tmp/debug_squad/checkpoint-4000/config.json\n",
            "01/14/2020 21:52:42 - INFO - transformers.modeling_utils -   Model weights saved in tmp/debug_squad/checkpoint-4000/pytorch_model.bin\n",
            "01/14/2020 21:52:42 - INFO - __main__ -   Saving model checkpoint to tmp/debug_squad/checkpoint-4000\n",
            "01/14/2020 21:52:46 - INFO - __main__ -   Saving optimizer and scheduler states to tmp/debug_squad/checkpoint-4000\n",
            "01/14/2020 22:01:23 - INFO - transformers.configuration_utils -   Configuration saved in tmp/debug_squad/checkpoint-5000/config.json\n",
            "01/14/2020 22:01:24 - INFO - transformers.modeling_utils -   Model weights saved in tmp/debug_squad/checkpoint-5000/pytorch_model.bin\n",
            "01/14/2020 22:01:24 - INFO - __main__ -   Saving model checkpoint to tmp/debug_squad/checkpoint-5000\n",
            "01/14/2020 22:01:27 - INFO - __main__ -   Saving optimizer and scheduler states to tmp/debug_squad/checkpoint-5000\n",
            "01/14/2020 22:10:04 - INFO - transformers.configuration_utils -   Configuration saved in tmp/debug_squad/checkpoint-6000/config.json\n",
            "01/14/2020 22:10:05 - INFO - transformers.modeling_utils -   Model weights saved in tmp/debug_squad/checkpoint-6000/pytorch_model.bin\n",
            "01/14/2020 22:10:05 - INFO - __main__ -   Saving model checkpoint to tmp/debug_squad/checkpoint-6000\n",
            "01/14/2020 22:10:09 - INFO - __main__ -   Saving optimizer and scheduler states to tmp/debug_squad/checkpoint-6000\n",
            "01/14/2020 22:18:46 - INFO - transformers.configuration_utils -   Configuration saved in tmp/debug_squad/checkpoint-7000/config.json\n",
            "01/14/2020 22:18:47 - INFO - transformers.modeling_utils -   Model weights saved in tmp/debug_squad/checkpoint-7000/pytorch_model.bin\n",
            "01/14/2020 22:18:47 - INFO - __main__ -   Saving model checkpoint to tmp/debug_squad/checkpoint-7000\n",
            "01/14/2020 22:18:50 - INFO - __main__ -   Saving optimizer and scheduler states to tmp/debug_squad/checkpoint-7000\n",
            "\n",
            "\n",
            "Epoch:  50%|█████     | 1/2 [1:04:50<1:04:50, 3890.82s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "40156f679b2f492ca15f2d8354ab5298",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Iteration', max=7470, style=ProgressStyle(description_width='…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "01/14/2020 22:27:27 - INFO - transformers.configuration_utils -   Configuration saved in tmp/debug_squad/checkpoint-8000/config.json\n",
            "01/14/2020 22:27:28 - INFO - transformers.modeling_utils -   Model weights saved in tmp/debug_squad/checkpoint-8000/pytorch_model.bin\n",
            "01/14/2020 22:27:28 - INFO - __main__ -   Saving model checkpoint to tmp/debug_squad/checkpoint-8000\n",
            "01/14/2020 22:27:32 - INFO - __main__ -   Saving optimizer and scheduler states to tmp/debug_squad/checkpoint-8000\n",
            "01/14/2020 22:36:08 - INFO - transformers.configuration_utils -   Configuration saved in tmp/debug_squad/checkpoint-9000/config.json\n",
            "01/14/2020 22:36:09 - INFO - transformers.modeling_utils -   Model weights saved in tmp/debug_squad/checkpoint-9000/pytorch_model.bin\n",
            "01/14/2020 22:36:09 - INFO - __main__ -   Saving model checkpoint to tmp/debug_squad/checkpoint-9000\n",
            "01/14/2020 22:36:12 - INFO - __main__ -   Saving optimizer and scheduler states to tmp/debug_squad/checkpoint-9000\n",
            "01/14/2020 22:44:49 - INFO - transformers.configuration_utils -   Configuration saved in tmp/debug_squad/checkpoint-10000/config.json\n",
            "01/14/2020 22:44:50 - INFO - transformers.modeling_utils -   Model weights saved in tmp/debug_squad/checkpoint-10000/pytorch_model.bin\n",
            "01/14/2020 22:44:50 - INFO - __main__ -   Saving model checkpoint to tmp/debug_squad/checkpoint-10000\n",
            "01/14/2020 22:44:54 - INFO - __main__ -   Saving optimizer and scheduler states to tmp/debug_squad/checkpoint-10000\n",
            "01/14/2020 22:53:31 - INFO - transformers.configuration_utils -   Configuration saved in tmp/debug_squad/checkpoint-11000/config.json\n",
            "01/14/2020 22:53:32 - INFO - transformers.modeling_utils -   Model weights saved in tmp/debug_squad/checkpoint-11000/pytorch_model.bin\n",
            "01/14/2020 22:53:32 - INFO - __main__ -   Saving model checkpoint to tmp/debug_squad/checkpoint-11000\n",
            "01/14/2020 22:53:36 - INFO - __main__ -   Saving optimizer and scheduler states to tmp/debug_squad/checkpoint-11000\n",
            "01/14/2020 23:02:12 - INFO - transformers.configuration_utils -   Configuration saved in tmp/debug_squad/checkpoint-12000/config.json\n",
            "01/14/2020 23:02:13 - INFO - transformers.modeling_utils -   Model weights saved in tmp/debug_squad/checkpoint-12000/pytorch_model.bin\n",
            "01/14/2020 23:02:13 - INFO - __main__ -   Saving model checkpoint to tmp/debug_squad/checkpoint-12000\n",
            "01/14/2020 23:02:17 - INFO - __main__ -   Saving optimizer and scheduler states to tmp/debug_squad/checkpoint-12000\n",
            "01/14/2020 23:10:53 - INFO - transformers.configuration_utils -   Configuration saved in tmp/debug_squad/checkpoint-13000/config.json\n",
            "01/14/2020 23:10:55 - INFO - transformers.modeling_utils -   Model weights saved in tmp/debug_squad/checkpoint-13000/pytorch_model.bin\n",
            "01/14/2020 23:10:55 - INFO - __main__ -   Saving model checkpoint to tmp/debug_squad/checkpoint-13000\n",
            "01/14/2020 23:10:58 - INFO - __main__ -   Saving optimizer and scheduler states to tmp/debug_squad/checkpoint-13000\n",
            "01/14/2020 23:19:35 - INFO - transformers.configuration_utils -   Configuration saved in tmp/debug_squad/checkpoint-14000/config.json\n",
            "01/14/2020 23:19:36 - INFO - transformers.modeling_utils -   Model weights saved in tmp/debug_squad/checkpoint-14000/pytorch_model.bin\n",
            "01/14/2020 23:19:36 - INFO - __main__ -   Saving model checkpoint to tmp/debug_squad/checkpoint-14000\n",
            "01/14/2020 23:19:40 - INFO - __main__ -   Saving optimizer and scheduler states to tmp/debug_squad/checkpoint-14000\n",
            "\n",
            "\n",
            "Epoch: 100%|██████████| 2/2 [2:09:42<00:00, 3891.00s/it]  \u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A01/14/2020 23:27:45 - INFO - __main__ -    global_step = 14941, average loss = 0.9776820335396322\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lvbrGLRwK0l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the trained model and the tokenizer\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(args.output_dir):\n",
        "    os.makedirs(args.output_dir)\n",
        "\n",
        "logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "# Take care of distributed/parallel training\n",
        "model_to_save = model.module if hasattr(model, \"module\") else model\n",
        "model_to_save.save_pretrained(args.output_dir)\n",
        "tokenizer.save_pretrained(args.output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_cnUPRf2S34",
        "colab_type": "code",
        "outputId": "d5e5bf0e-aa2e-457e-d331-6475a9b5de0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Load a trained model and vocabulary that you have fine-tuned\n",
        "model = model_class.from_pretrained(args.output_dir)  # , force_download=True)\n",
        "tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=True)\n",
        "model.to(args.device)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "01/15/2020 15:43:50 - INFO - transformers.configuration_utils -   loading configuration file output/config.json\n",
            "01/15/2020 15:43:50 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "01/15/2020 15:43:50 - INFO - transformers.modeling_utils -   loading weights file output/pytorch_model.bin\n",
            "01/15/2020 15:44:05 - INFO - transformers.tokenization_utils -   Model name 'output/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming 'output/' is a path or url to a directory containing tokenizer files.\n",
            "01/15/2020 15:44:05 - INFO - transformers.tokenization_utils -   loading file output/vocab.txt\n",
            "01/15/2020 15:44:05 - INFO - transformers.tokenization_utils -   loading file output/added_tokens.json\n",
            "01/15/2020 15:44:05 - INFO - transformers.tokenization_utils -   loading file output/special_tokens_map.json\n",
            "01/15/2020 15:44:05 - INFO - transformers.tokenization_utils -   loading file output/tokenizer_config.json\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForQuestionAnswering(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOXDvcgpi6YV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(args, model, tokenizer, prefix=\"\"):\n",
        "    dataset, examples, features = load_and_cache_examples(args, tokenizer, evaluate=True, output_examples=True)\n",
        "\n",
        "    if not os.path.exists(args.output_dir):\n",
        "        os.makedirs(args.output_dir)\n",
        "\n",
        "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
        "\n",
        "    # Note that DistributedSampler samples randomly\n",
        "    eval_sampler = SequentialSampler(dataset)\n",
        "    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "    # Eval!\n",
        "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
        "    logger.info(\"  Num examples = %d\", len(dataset))\n",
        "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
        "\n",
        "    all_results = []\n",
        "    start_time = timeit.default_timer()\n",
        "\n",
        "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "        model.eval()\n",
        "        batch = tuple(t.to(args.device) for t in batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"attention_mask\": batch[1],\n",
        "                \"token_type_ids\": batch[2],\n",
        "            }\n",
        "\n",
        "            example_indices = batch[3]\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        for i, example_index in enumerate(example_indices):\n",
        "            eval_feature = features[example_index.item()]\n",
        "            unique_id = int(eval_feature.unique_id)\n",
        "\n",
        "            output = [to_list(output[i]) for output in outputs]\n",
        "\n",
        "            start_logits, end_logits = output\n",
        "            result = SquadResult(unique_id, start_logits, end_logits)\n",
        "\n",
        "            all_results.append(result)\n",
        "\n",
        "    evalTime = timeit.default_timer() - start_time\n",
        "    logger.info(\"  Evaluation done in total %f secs (%f sec per example)\", evalTime, evalTime / len(dataset))\n",
        "\n",
        "    # Compute predictions\n",
        "    output_prediction_file = os.path.join(args.output_dir, \"predictions_{}.json\".format(prefix))\n",
        "    output_nbest_file = os.path.join(args.output_dir, \"nbest_predictions_{}.json\".format(prefix))\n",
        "\n",
        "    output_null_log_odds_file = None\n",
        "\n",
        "    predictions = compute_predictions_logits(\n",
        "        examples,\n",
        "        features,\n",
        "        all_results,\n",
        "        args.n_best_size,\n",
        "        args.max_answer_length,\n",
        "        do_lower_case=False,\n",
        "        output_prediction_file=output_prediction_file,\n",
        "        output_nbest_file=output_nbest_file,\n",
        "        output_null_log_odds_file=output_null_log_odds_file,\n",
        "        verbose_logging=False,\n",
        "        version_2_with_negative=False,\n",
        "        null_score_diff_threshold=0.0,\n",
        "    )\n",
        "\n",
        "    # Compute the F1 and exact scores.\n",
        "    results = squad_evaluate(examples, predictions)\n",
        "    return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjdGxMZ06kem",
        "colab_type": "code",
        "outputId": "c176a9d5-7a1c-4d77-b4aa-a64045ecd528",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 910,
          "referenced_widgets": [
            "b58fd82446a143c18f2ef954693b80ef",
            "014acd80b7ab4a6d8c2d375eb7615972",
            "fdc62dfb7ece40649976fb3e4acaa12b",
            "8275eb400e7543cebce9dbc031b66e81",
            "877bb321aa8a466491b072f05691abe7",
            "2e2b2c724f5a4afa93bb7fbfb623eca2",
            "690b2c84c6d0429caddfa9ba8e5eed23",
            "4efeb67541e24d3db5f2bfc31e5e4033"
          ]
        }
      },
      "source": [
        "# Evaluate\n",
        "result = evaluate(args, model, tokenizer)\n",
        "logger.info(\"Results: {}\".format(result))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "01/14/2020 23:57:28 - INFO - __main__ -   Loading features from cached file ./cached_dev_bert-base-cased_384\n",
            "01/14/2020 23:57:31 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "01/14/2020 23:57:31 - INFO - __main__ -     Num examples = 10970\n",
            "01/14/2020 23:57:31 - INFO - __main__ -     Batch size = 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b58fd82446a143c18f2ef954693b80ef",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Evaluating', max=1372, style=ProgressStyle(description_width=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "01/15/2020 00:00:14 - INFO - __main__ -     Evaluation done in total 162.810710 secs (0.014841 sec per example)\n",
            "01/15/2020 00:00:14 - INFO - transformers.data.metrics.squad_metrics -   Writing predictions to: tmp/debug_squad/predictions_.json\n",
            "01/15/2020 00:00:14 - INFO - transformers.data.metrics.squad_metrics -   Writing nbest to: tmp/debug_squad/nbest_predictions_.json\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "01/15/2020 00:00:55 - INFO - __main__ -   Results: {'exact': 80.26490066225166, 'f1': 87.99811557687403, 'total': 10570, 'HasAns_exact': 80.26490066225166, 'HasAns_f1': 87.99811557687403, 'HasAns_total': 10570, 'best_exact': 80.26490066225166, 'best_exact_thresh': 0.0, 'best_f1': 87.99811557687403, 'best_f1_thresh': 0.0}\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eqnk2Xybyzy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(args,tokenizer,model,q,doc,device):\n",
        "    indexed_tokens = tokenizer.encode(q,doc)\n",
        "    seg_idx = indexed_tokens.index(102)+1\n",
        "    attention_mask = [1]*len(indexed_tokens)\n",
        "    segment_ids = [0]*seg_idx+[1]*(len(indexed_tokens)-seg_idx)\n",
        "    indexed_tokens += [0]*(args.max_seq_length-len(indexed_tokens))\n",
        "    attention_mask += [0]*(args.max_seq_length-len(attention_mask))\n",
        "    segment_ids += [0]*(args.max_seq_length-len(segment_ids))\n",
        "    \n",
        "    # for debugging\n",
        "    # ind2word = {v:k for k,v in tokenizer.vocab.items()}\n",
        "    # [ind2word[ind] for ind in indexed_tokens]\n",
        "\n",
        "    tokens_tensor = torch.tensor([indexed_tokens]).to(device)\n",
        "    segment_tensor = torch.tensor([segment_ids]).to(device)\n",
        "    attention_tensor = torch.tensor([attention_mask]).to(device)\n",
        "\n",
        "    # Predict the start and end positions logits\n",
        "    with torch.no_grad():\n",
        "        start_logits, end_logits = model(tokens_tensor, token_type_ids=segment_tensor, attention_mask=attention_tensor)\n",
        "\n",
        "    # get the highest prediction\n",
        "    answer = tokenizer.decode(indexed_tokens[torch.argmax(start_logits):torch.argmax(end_logits)+1])\n",
        "    return answer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdOISimCeLga",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "c46e0823-3de7-485d-d496-9b81c9226486"
      },
      "source": [
        "doc = \"Jim Henson was a puppeteer\"\n",
        "q = \"Who was Jim Henson?\"\n",
        "ans = predict(args,tokenizer,model,q,doc,device)\n",
        "print(ans)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a puppeteer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsNkufz8wg5W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "e29505cf-68f2-45f6-aa0f-71ff5c9a1902"
      },
      "source": [
        "doc = \"Once upon a time there was an old mother pig who had three little pigs and not enough food to feed them. So when they were old enough, she sent them out into the world to seek their fortunes. The first little pig was very lazy. He didn't want to work at all and he built his house out of straw. The second little pig worked a little bit harder but he was somewhat lazy too and he built his house out of sticks. Then, they sang and danced and played together the rest of the day. The third little pig worked hard all day and built his house with bricks. It was a sturdy house complete with a fine fireplace and chimney. It looked like it could withstand the strongest winds. The next day, a wolf happened to pass by the lane where the three little pigs lived; and he saw the straw house, and he smelled the pig inside. He thought the pig would make a mighty fine meal and his mouth began to water.\"\n",
        "q = 'What did the pigs do for the rest of the day?'\n",
        "ans = predict(args,tokenizer,model,q,doc,device)\n",
        "print(ans)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sang and danced and played together\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xdz96664HEw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}